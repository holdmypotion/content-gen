<post_1>
How LinkedIn serves the feed you are scrolling right now ?

You are reading this post. But how did it get here? LinkedIn has 1 billion members. If I post this, how does the system know to show it to you specifically within milliseconds?

Itâ€™s not just one database. Itâ€™s a specialized search engine called Galene.

The "Inverted Index" Search engines usually index documents (like Google indexing web pages). But LinkedIn has to index People and Entities. And unlike a web page, your profile changes constantly. You connect, you like, you comment.

The Writ: When I hit "Post," it goes to a distributed Key-Value store (Espresso).
The Stream: That write triggers a Kafka event.
The Index: Galene consumes that Kafka stream and updates the index in near real-time (NRT).

To make the feed fast, Galene doesn't search everyone. It uses Early Termination.

It ranks the potential matches based on a lightweight score (affinity, recency). Once it finds "enough" good posts (e.g., the top 100), it stops searching. It trades "Perfect Accuracy" for "Low Latency."

Great search engineering isn't about finding every result. Itâ€™s about finding the good enough results fast enough to keep the user scrolling.

#LinkedInEngineering #SearchEngine #Kafka #SystemDesign #BigData
</post_1>

<post_2>
MySQL broke at Uber. Here is how they fixed it.

When you run at Uber's scale (Petabytes of data, millions of QPS), the textbook answer stops working.

Uber just revealed that their standard MySQL High Availability (HA) setup was causing too much downtime.

The Problem: "Reactive" Failover
For years, Uber used a classic setup:

â€¢ A primary node takes writes.
â€¢ A secondary node replicates them (asynchronously).
â€¢ An external controller watches for failure.
If the primary died, the controller had to notice, decide, and promote the secondary.

The result? Data loss (lag) and seconds of downtime during the switch.
The Solution: "Consensus" (MySQL Group Replication)

Uber Engineering didn't just patch the old system. They moved to a Consensus-based Architecture using MySQL Group Replication (MGR).

Instead of an external "manager" deciding who is alive, the database nodes now talk to each other using Paxos.

The "Real" Engineering Challenge
Here is the part most blogs leave out. You can't just "turn on" MGR and walk away.

Uber found that at their scale, network jitters caused "flapping" (nodes joining/leaving the group constantly).

To fix this, they built a custom Consensus Layer on top:

1. Single-Primary Mode: They strictly enforce one writer to avoid conflict complexity.

2. Advanced Detection: They added logic to distinguish between a "blip" and a "crash" before triggering a view change.

3. Graceful Offboarding: Nodes are removed from the consensus group before maintenance to prevent performance degradation.

The Lesson
"High Availability" isn't a setting you toggle in your cloud provider.
It is an architectural decision between Eventual Consistency (fast, risky) and Consensus (complex, robust).
Uber chose complexity to buy reliability.
</post_2>

<post_3>
Here's something interesting about cache eviction - instead of tracking the exact LRU (Least Recently Used) item, we can just pick two random items and evict the older one.

This is called "2-random," and it works surprisingly well.

The idea is simple. When we need to evict something from our cache, randomly sample two keys, compare their last access times, and evict the one that was accessed longer ago. That's it.

Why not just track true LRU? because, maintaining a perfect LRU requires extra memory and CPU overhead. We need a doubly linked list with pointers for every cache entry, plus the cost of updating it on every access.

Btw, picking two keys at random and evicting the least recently used of the two becomes virtually indistinguishable from true LRU. It works because it avoids the worst random choices (by picking the better of two) while retaining enough randomness.

2-random also degrades gracefully when your working set exceeds the cache size. True LRU can cause near-100% cache misses when looping over data larger than the cache. Random eviction handles this better, and 2-random gives you the best of both worlds.

Many in-memory databases are full of such "good enough" approximations. Dig deeper when you find time. It's fun.
</post_3>

<post_4>
If you drop a packet on Zoom, the video glitches. If you drop a packet on Netflix, you never notice.
Why? Because they are solving two completely different physics problems.

1. The "Real-Time" Problem (Zoom/WebRTC) In a live conversation, Latency is king. You cannot "buffer" a conversation. If I say "Hello," you need to hear it now.
Protocol: UDP.
Strategy: If a packet is lost, ignore it. Do not retransmit. It's better to have a glitchy frame than a 2-second delay.

2. The "On-Demand" Problem (Netflix/YouTube) In a movie, Quality is king. You don't care if the video frame you are watching arrived 5 milliseconds ago or 5 minutes ago, as long as it looks perfect (4K).
Protocol: TCP (via HTTP/DASH).
Strategy: Buffering. Netflix downloads the next 5 minutes of the movie ahead of time.

If a packet is lost? TCP retransmits it.

Does the video freeze? No. The player just reads from the buffer while TCP fixes the error in the background.

The Evolution: QUIC (The Best of Both) ðŸš€ Recently, Netflix (and YouTube) have started moving to QUIC. QUIC is technically built on top of UDP, but it implements the reliability and congestion control of TCP in "User Space."
It gives them the reliable delivery of TCP without the "Head-of-Line Blocking" latency issues.

The Takeaway: Don't memorize "Video uses UDP." Understand the constraint: Is the user interacting (UDP)? or Consuming (TCP)?

#SystemDesign #Netflix #Streaming #TCP #UDP #Engineering
</post_4>

<post_5>
PostgreSQL lets you clone a 6GB database in 212 milliseconds instead of 67 seconds. Here's how...

Cloning databases comes in handy in a few situations:

- testing migration without touching prod data
- spinning up fresh copies for each test suite run
- resetting sandbox env between sessions
- reproducible snapshots for debugging

When your database is a few megabytes, pg_dump works fine. With hundreds of gigabytes, "just make a copy" becomes a serious bottleneck.

PostgreSQL has always had a templating system. Every `CREATE DATABASE` quietly clones `template1` behind the scenes, and you can replace `template1` with any database. (wrote about it earlier in one of my posts)

Version 15 introduced the `STRATEGY` parameter, switching to WAL_LOG by default (block-by-block copy via Write-Ahead Log). Smoother I/O, but slower for large databases.

PostgreSQL 18 has an option `file_copy_method = clone`. On modern filesystems like XFS, ZFS, or APFS, this leverages the FICLONE operation. Instead of copying bytes, the file system creates new metadata that points to the same physical blocks. Both databases share identical storage until you write something.

Here, the supported File System is doing the magic, which creates a copy-on-write (CoW) clone of a file.

When we update a row, the filesystem triggers copy-on-write only for the affected pages. The rest stays shared. 6GB clone takes zero additional space initially and grows only as data diverges.

One thing to keep in mind: the source database can't have active connections during cloning. This is a PostgreSQL limitation, not a filesystem one.

Pretty neat!
</post_5>

<post_6>
When Pinterest hit 100 million users, their MySQL cluster was dying. The problem wasn't the data size. It was the complexity of the relationships.
Users follow Boards. Boards have Pins. Pins have Likes. A single page load required massive JOIN operations across tables with billions of rows. 

So they did the unthinkable: They deleted all Foreign Keys and disabled Joins.
The Solution: Manually Sharded MySQL (The "Spotlight" Architecture) 

Instead of letting the database manage relationships, they moved that logic to the Application Layer.

1. The "Universal ID" Hack: They designed a custom 64-bit ID structure that tells you exactly where data lives without looking it up.
Bits 0-15: Shard ID (Which physical server?)
Bits 16-25: Type ID (Is this a Pin? A Board? A User?)
Bits 26-64: Local ID (Auto-increment in that specific table).

The Magic: If I give you ID 2495349, you don't need a central index to find it. You just parse the first 16 bits -> "Oh, this is on Shard #40." -> Connect to Shard #40 -> SELECT * FROM pins WHERE id = ...

2. The Mapping Tables: Since they can't JOIN tables across different shards, they store relationships in separate "Mapping Tables" that act like a massive adjacency list.
Table: board_has_pins
board_id (Key)
pin_id
sequence_id (For ordering)

To render a Board:
Query 1: Go to the Board Shard -> Get the list of pin_ids from the mapping table.
Query 2: Parse those IDs to find which shards they live on.
Query 3: Fire parallel queries to all those shards to fetch the actual Pin data.

The Takeaway: Relational databases are great at relationshipsâ€”until you shard. Once you shard, your database is just a dumb storage engine. Your code becomes the database.

But, this was 2013. If you had to do this today, you would use some modern ways to get past these issues. Let me know in the comments, what will you use ?

#SystemDesign #Pinterest #MySQL #Sharding #Database #Scalability
</post_6>

<post_7>
If you are building a distributed cache or sharding a database, your first instinct is usually Modulo Hashing.

server_index = hash(key) % N

It looks perfect. It spreads traffic evenly across your N servers.

Until one server crashes. 

The "Rebalancing Storm":
When N changes to N-1 (because a server died) or N+1 (because you scaled up), the math changes for every single key.
â€¢ Key A: hash(A) % 5 = 2 (Server 2)
â€¢ Key A: hash(A) % 6 = 4 (Server 4) 

The Math:
In a Modulo system, changing the server count reshuffles ~100% of your data.
Your cache hit ratio drops to zero instantly. Your database gets hammered by the "Thundering Herd."
The Solution: Consistent Hashing (The Ring)

Invented by Akamai (and popularized by Amazon Dynamo), this treats your servers as points on a circle (0Â° to 360Â°).

1. The Ring: Map the integers 0 to 2^{32} on a circle.
2. The Servers: Hash your Server IPs to place them at random points on the ring.
3. The Keys: Hash your data keys to points on the ring.
4. The Lookup: To find where a key lives, walk clockwise on the ring until you hit the first server.

Why this is genius:

- When you add a new server to the ring, you don't reshuffle the whole world.
- You only "steal" a small slice of keys from the server immediately ahead of you. 
â€¢ Modulo Hashing: 100% data movement.
â€¢ Consistent Hashing: 1/N data movement.

Scalability isn't just about handling traffic. It's about handling topology changes gracefully.
If scaling up causes downtime, you didn't scale; you just rebooted.
#SystemDesign #Algorithms #DistributedSystems #DynamoDB #Cassandra
</post_7>
